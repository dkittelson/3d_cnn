"""
DOMAIN ADVERSARIAL TRAINING (DANN)
==============================
Addresses donor bias: D1-D3 (small/inactive) vs D4-D5 (large/active)

Model learns to:
  1. Classify activity (main task)
  2. Be invariant to donor identity (adversarial task)

Architecture:
  Input â†’ Feature Extractor â†’ Activity Classifier (active/inactive)
                           â””â†’ GRL â†’ Donor Classifier (D1-D6)

Gradient Reversal Layer (GRL) forces features to be:
  â€¢ Discriminative for activity
  â€¢ Invariant to donor (cell size, texture)

Expected improvement: +2-5% accuracy
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
from pathlib import Path
import json
from datetime import datetime
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import torch.nn.functional as F
from torch.amp import autocast, GradScaler  
from torch.utils.data import WeightedRandomSampler
import pickle
import hashlib

# Enable TF32 for faster matmul on A100 GPUs (10-20% speedup)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# Import DANN model
from resnet_dann import create_dann_model
print("ðŸ”¥ Using DANN (Domain Adversarial Neural Network) - Target: 87-90% accuracy")

# ============================================================================
# MIXUP DATA AUGMENTATION
# ============================================================================
def mixup_data(x, y, alpha=0.4):
    """Returns mixed inputs, pairs of targets, and lambda
    
    Mixup creates virtual training examples by linearly interpolating between
    random pairs of samples. This smooths the decision boundary and prevents
    the model from memorizing donor-specific textures.
    
    Args:
        x: Input batch (B, C, T, H, W)
        y: Labels (B,)
        alpha: Beta distribution parameter (0.4 is optimal from literature)
    
    Returns:
        mixed_x: Interpolated inputs
        y_a, y_b: Original label pairs
        lam: Interpolation coefficient
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Compute mixup loss as weighted combination of two labels"""
    return lam * criterion(pred, y_a.unsqueeze(1)) + (1 - lam) * criterion(pred, y_b.unsqueeze(1))

# Cache directory for preprocessed data
CACHE_DIR = Path('./cache')
CACHE_DIR.mkdir(exist_ok=True)

# ============================================================================
# DATASET CLASS
# ============================================================================
GLOBAL_INTENSITY_MIN = 0.0
GLOBAL_INTENSITY_MAX = 8.2212  # Actual max from dataset - provides optimal contrast for dim cells

class CellDataset(Dataset):
    """Custom Dataset that loads data on-the-fly (with donor labels for DANN)"""

    def __init__(self, file_paths, labels, augment=False, donor_labels=None):
        self.file_paths = file_paths
        self.labels = labels
        self.augment = augment
        self.donor_labels = donor_labels  # D1=0, D2=1, D3=2, D4=3, D5=4, D6=5
    
    def __len__(self):
        return len(self.file_paths)
    
    def apply_augmentation(self, array):
        """Apply random augmentations to 3D array"""
        
        # ===== TEMPORAL JITTER (IRF Drift Correction) =====
        # FLIM hardware drift: laser trigger timing shifts between donors
        # This causes the decay peak to shift by 2-3 time bins
        # Randomly shift time axis to make model learn curve SHAPE, not absolute position
        if np.random.rand() > 0.2:  # 80% probability - very important
            shift = np.random.randint(-5, 6)  # Â±5 time bins (~2-3 typical drift)
            if shift != 0:
                array = np.roll(array, shift, axis=2)
                # Zero out wrap-around artifacts
                if shift < 0:
                    array[:, :, shift:] = 0  # Zero out end
                elif shift > 0:
                    array[:, :, :shift] = 0  # Zero out start
        
        # ===== INTENSITY JITTER (Domain Shift) =====
        # D5 is dimmer than D1-4 training donors
        # Scale brightness to make model brightness-invariant
        if np.random.rand() > 0.2:  # 80% probability
            scale_factor = np.random.uniform(0.6, 1.4)  # Â±40% brightness
            array = array * scale_factor
        
        # Rotations
        if np.random.rand() > 0.5:
            k = np.random.choice([1, 2, 3])
            array = np.rot90(array, k=k, axes=(0, 1)).copy()
        
        # Flips
        if np.random.rand() > 0.5:
            array = np.flip(array, axis=0).copy()  
        if np.random.rand() > 0.5:
            array = np.flip(array, axis=1).copy()  
        
        # Poisson Noise (photon shot noise - realistic for FLIM)
        if np.random.rand() > 0.5:
            # Scale up to simulate photon counts, add Poisson noise, scale back
            scale = 1000  # Simulate ~1000 photons per voxel on average
            photon_counts = array * scale
            noisy_counts = np.random.poisson(photon_counts)
            array = noisy_counts / scale

        # Spatial Scaling
        if np.random.rand() > 0.5:
            from scipy.ndimage import zoom
            scale_factor = np.random.uniform(0.85, 1.15)
            zoomed = zoom(array, (scale_factor, scale_factor, 1.0), order=1)
            h, w, t = array.shape
            zh, zw, _ = zoomed.shape
            if zh > h: # Crop
                start_h = (zh - h) // 2
                zoomed = zoomed[start_h:start_h+h, :, :]
            elif zh < h: # Pad
                pad_h = (h - zh) // 2
                zoomed = np.pad(zoomed, ((pad_h, h-zh-pad_h), (0, 0), (0, 0)), 
                          mode='constant', constant_values=0)
            if zw > w: # Crop
                start_w = (zw - w) // 2
                zoomed = zoomed[:, start_w: start_w+w, :]
            elif zw < w: # Pad
                pad_w = (w - zw) // 2
                zoomed = np.pad(zoomed, ((0, 0), (pad_w, w-zw-pad_w), (0, 0)), 
                          mode='constant', constant_values=0)
            array = zoomed

        # Elastic Deformation
        if np.random.rand() > 0.5:
            from scipy.ndimage import gaussian_filter, map_coordinates
            alpha = 5 # Deformation strength
            sigma = 2 # Smoothness
            random_state = np.random.RandomState(None)
            dx = gaussian_filter((random_state.rand(array.shape[0], array.shape[1]) * 2 - 1), 
                            sigma, mode="constant", cval=0) * alpha
            dy = gaussian_filter((random_state.rand(array.shape[0], array.shape[1]) * 2 - 1), 
                            sigma, mode="constant", cval=0) * alpha
            x, y = np.meshgrid(np.arange(array.shape[1]), np.arange(array.shape[0]))
            indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))
            deformed = np.zeros_like(array)
            for t in range(array.shape[2]):
                deformed[:, :, t] = map_coordinates(array[:, :, t], indices, 
                                               order=1, mode='reflect').reshape(array.shape[:2])
            array = deformed

        # Gaussian Blur
        if np.random.rand() > 0.5:
            from scipy.ndimage import gaussian_filter
            sigma = np.random.uniform(0.3, 0.8)
            # Only blur spatial dimensions
            for t in range(array.shape[2]):
                array[:, :, t] = gaussian_filter(array[:, :, t], sigma=sigma)

        return np.ascontiguousarray(array)
    
    def __getitem__(self, idx):
        import hashlib
        from pathlib import Path
        import tempfile
        import shutil
        
        file_path = self.file_paths[idx]
        
        # Create cache key based on file path
        cache_dir = Path('./cache')
        cache_key = hashlib.md5(str(file_path).encode()).hexdigest()
        cache_file = cache_dir / f"{cache_key}.pkl"
        
        # Try to load from cache (only for non-augmented data)
        if not self.augment and cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    array = pickle.load(f)
            except (pickle.UnpicklingError, EOFError, OSError):
                # Cache file is corrupted, delete and reload
                cache_file.unlink(missing_ok=True)
                array = np.load(file_path)
                # Re-cache with atomic write
                try:
                    with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=cache_dir, suffix='.tmp') as tmp_f:
                        pickle.dump(array, tmp_f)
                        tmp_path = tmp_f.name
                    shutil.move(tmp_path, cache_file)
                except Exception:
                    # If caching fails, just continue without cache
                    if Path(tmp_path).exists():
                        Path(tmp_path).unlink(missing_ok=True)
        else:
            # Load file
            array = np.load(file_path)
            
            # Cache the raw array if not augmenting (atomic write)
            if not self.augment:
                try:
                    with tempfile.NamedTemporaryFile(mode='wb', delete=False, dir=cache_dir, suffix='.tmp') as tmp_f:
                        pickle.dump(array, tmp_f)
                        tmp_path = tmp_f.name
                    shutil.move(tmp_path, cache_file)
                except Exception:
                    # If caching fails, just continue without cache
                    if 'tmp_path' in locals() and Path(tmp_path).exists():
                        Path(tmp_path).unlink(missing_ok=True)
        
        if self.augment:
            array = self.apply_augmentation(array)
        
        # Sum across time axis
        total_photons_per_pixel = np.sum(array, axis=2, keepdims=True)
        total_photons_per_pixel = np.where(
            total_photons_per_pixel == 0, 
            1.0,
            total_photons_per_pixel
        )

        # Channel 0: Normalized decay shape (divide by SUM for stability with low photon counts)
        # Peak norm amplifies noise by 26% (sqrt(14)/14), sum norm only 1% (sqrt(10k)/10k)
        decay_normalized = np.zeros_like(array, dtype=np.float32)
        for i in range(array.shape[0]):
            for j in range(array.shape[1]):
                pixel_sum = array[i, j, :].sum()
                if pixel_sum > 0:
                    decay_normalized[i, j, :] = array[i, j, :] / pixel_sum

        # Channel 1: Anscombe-transformed intensity map (variance stabilization for Poisson noise)
        intensity_map = array.sum(axis=2)  # Total photons per pixel
        
        # Anscombe transform: 2*sqrt(x + 3/8) stabilizes Poisson variance
        anscombe_intensity = 2 * np.sqrt(intensity_map + 3/8)
        
        # Normalize to [0, 1] using global statistics
        anscombe_intensity = (anscombe_intensity - GLOBAL_INTENSITY_MIN) / (GLOBAL_INTENSITY_MAX - GLOBAL_INTENSITY_MIN)
        anscombe_intensity = np.clip(anscombe_intensity, 0, 1)  # Ensure [0, 1] range

        # Expand intensity to match temporal dimensions
        anscombe_intensity_expanded = np.tile(anscombe_intensity[:, :, np.newaxis], (1, 1, array.shape[2]))

        # Tranpose to (Time, Height, Width) for CNN
        decay_normalized = np.transpose(decay_normalized, (2, 0, 1))
        anscombe_intensity_expanded = np.transpose(anscombe_intensity_expanded, (2, 0, 1))

        # Stack as 2 channels (2, 256, 21, 21)
        array = np.stack([decay_normalized, anscombe_intensity_expanded], axis=0)

        # Convert to PyTorch tensors 
        X = torch.tensor(array, dtype=torch.float32)
        y = torch.tensor(self.labels[idx], dtype=torch.float32)
        
        # Return with donor label if available
        if self.donor_labels is not None:
            donor = torch.tensor(self.donor_labels[idx], dtype=torch.long)
            return X, y, donor
        else:
            return X, y

# ============================================================================
# EXTRACT DONOR ID FROM FILE PATH
# ============================================================================

def extract_donor_id(file_path):
    """
    Extract donor ID from file path
    
    Examples:
      isolated_cells_D1/file.npy â†’ 0
      isolated_cells_D2/file.npy â†’ 1
      ...
      isolated_cells_D6/file.npy â†’ 5
    """
    path_str = str(file_path)
    for i in range(1, 7):
        if f'isolated_cells_D{i}' in path_str:
            return i - 1  # D1=0, D2=1, ..., D6=5
    raise ValueError(f"Could not extract donor ID from {file_path}")


# ============================================================================
# DATA COLLECTION BY FOLDER
# ============================================================================

def collect_data_by_folder():
    """Collects all data files by their dataset folder (D1-D6)"""

    data_dir = Path("/content/3d_cnn/3d_cnn/data")
    dataset_dict = {}

    for i in range(1, 7):

        # Get folder name
        folder_name = f'isolated_cells_D{i}'

        # Find folder 
        folder = data_dir / folder_name
        if not folder.exists():
            print(f"Warning: {folder_name} does not exist, skipping...")
            continue

        # Empty list for files and labels
        files = []
        labels = []

        # Iterate through each folder
        for file in folder.glob('*.npy'):
            
            # Check each label
            if '_in' in file.name:
                label = 0
            elif '_act' in file.name:
                label = 1
            else:
                continue

            files.append(file)
            labels.append(label)

        # Store files, labels in dict with folder_name as key
        dataset_dict[folder_name] = (files, labels)
        print(f"{folder_name}: {len(files)} files (inactive={labels.count(0)}, active={labels.count(1)})")
    
    return dataset_dict


# ============================================================================
# CREATE TRAIN/VALIDATION SPLIT FOR ONE FOLD
# ============================================================================

def create_fold_data(dataset_dict, val_fold):
    """Splits data into train and validation sets for a specific fold"""
    
    # Create empty lists
    train_files = []
    train_labels = []
    val_files = []
    val_labels = []
    
    # Loop through each dataset in the dict
    for fold_name, (files, labels) in dataset_dict.items():
        if fold_name == val_fold:
            # This fold is the validation/test set
            val_files.extend(files)
            val_labels.extend(labels)
        else:
            # All other folds are training set
            train_files.extend(files)
            train_labels.extend(labels)
        
    return train_files, train_labels, val_files, val_labels


# ============================================================================
# TRAIN ONE FOLD
# ============================================================================

def train_one_fold(fold_name, train_files, train_labels, val_files, val_labels, test_files, test_labels,
                   device, num_epochs=20, patience=5, batch_size=16, learning_rate=0.001, weight_decay=0):
    """Trains and evaluates a single fold"""
    
    # Print fold header and data summary
    print(f"\n{'='*80}")
    print(f"TRAINING FOLD: {fold_name}")
    print(f"{'='*80}")
    print(f"Training samples: {len(train_files)}")
    print(f"  - Inactive: {train_labels.count(0)} ({train_labels.count(0)/len(train_labels)*100:.1f}%)")
    print(f"  - Active:   {train_labels.count(1)} ({train_labels.count(1)/len(train_labels)*100:.1f}%)")
    print(f"Validation samples: {len(val_files)}")
    print(f"  - Inactive: {val_labels.count(0)} ({val_labels.count(0)/len(val_labels)*100:.1f}%)")
    print(f"  - Active:   {val_labels.count(1)} ({val_labels.count(1)/len(val_labels)*100:.1f}%)")
    
    # Extract donor IDs for DANN training
    train_donor_labels = [extract_donor_id(f) for f in train_files]
    val_donor_labels = [extract_donor_id(f) for f in val_files]
    
    # Create CellDataset objects for train and validation (with donor labels)
    train_dataset = CellDataset(train_files, train_labels, augment=True, donor_labels=train_donor_labels)
    val_dataset = CellDataset(val_files, val_labels, augment=False, donor_labels=val_donor_labels)

    # Print class distribution
    print(f"\nðŸ“Š Class distribution:")
    active_count = sum(1 for label in train_labels if label == 1)
    inactive_count = len(train_labels) - active_count
    print(f"  - Active cells: {active_count} ({active_count/len(train_labels)*100:.1f}%)")
    print(f"  - Inactive cells: {inactive_count} ({inactive_count/len(train_labels)*100:.1f}%)") 
    print(f"  - Using WeightedRandomSampler for balanced 50/50 batches")
    print(f"  - NO pos_weight in loss (sampler handles imbalance)\n")

    # WeightedRandomSampler for TRAINING - balanced 50/50 batches
    train_sample_weights = []
    active_count_total = sum(1 for label in train_labels if label == 1)
    inactive_count_total = len(train_labels) - active_count_total
    
    for label in train_labels:
        if label == 1:  # Active
            weight = 1.0 / active_count_total
        else:  # Inactive  
            weight = 1.0 / inactive_count_total
        train_sample_weights.append(weight)
    
    train_sampler = WeightedRandomSampler(
        weights=train_sample_weights,
        num_samples=len(train_dataset),
        replacement=True
    )

    # WeightedRandomSampler for VALIDATION - CRITICAL FIX!
    # D6 is 77% inactive, but we force 50/50 batches during validation
    # This prevents model from learning "guess inactive" strategy
    val_sample_weights = []
    val_active_count = sum(1 for label in val_labels if label == 1)
    val_inactive_count = len(val_labels) - val_active_count
    
    for label in val_labels:
        if label == 1:  # Active
            weight = 1.0 / val_active_count
        else:  # Inactive
            weight = 1.0 / val_inactive_count
        val_sample_weights.append(weight)
    
    val_sampler = WeightedRandomSampler(
        weights=val_sample_weights,
        num_samples=len(val_dataset),
        replacement=True
    )

    print(f"  ðŸŽ¯ BALANCED VALIDATION SAMPLING ENABLED")
    print(f"     â€¢ D6 raw distribution: {val_inactive_count} inactive ({val_inactive_count/len(val_labels)*100:.1f}%), {val_active_count} active ({val_active_count/len(val_labels)*100:.1f}%)")
    print(f"     â€¢ But validation batches will be 50/50 (prevents 'guess inactive' bias)")
    print(f"     â€¢ This forces model to learn features that work on D5 (67% active)\n")

    # Create DataLoaders with WeightedRandomSampler
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        sampler=train_sampler,  # Balanced training batches
        num_workers=4, 
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=True,  
        prefetch_factor=2  
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        sampler=val_sampler,  # ðŸ”¥ BALANCED validation batches (key fix!)
        num_workers=4,
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=True,
        prefetch_factor=2
    )

    # Create fresh DANN model
    model = create_dann_model(in_channels=2, lambda_grl=0.1).to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) 

    # Initialize gradient scaler for mixed precision
    scaler = GradScaler(device.type)  
    
    # Use standard cosine annealing (NO restarts for DANN stability)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs,    # Decay smoothly over full training
        eta_min=1e-6         # Minimum LR at end
    )
    print("  âœ“ Using CosineAnnealingLR (stable decay, no restarts for DANN)")

    # DANN Loss functions
    activity_loss_fn = nn.BCEWithLogitsLoss()  # Binary classification (active/inactive)
    donor_loss_fn = nn.CrossEntropyLoss()      # Multi-class classification (D1-D6)
    
    # Loss weights
    lambda_donor = 0.1  # Weight for donor loss (smaller to focus on activity)
    label_smoothing = 0.1
    
    print(f"  âœ“ DANN Loss initialized")
    print(f"    â€¢ Activity loss: BCEWithLogitsLoss + Label smoothing ({label_smoothing})")
    print(f"    â€¢ Donor loss: CrossEntropyLoss (6 classes: D1-D6)")
    print(f"    â€¢ Lambda_donor: {lambda_donor} (gradient reversal strength)")
    print(f"    â€¢ Mixup (alpha=0.4): interpolates between samples for smooth boundaries")
    print(f"    â€¢ Dataset: {active_count} active ({active_count/len(train_labels)*100:.1f}%), {inactive_count} inactive ({inactive_count/len(train_labels)*100:.1f}%)")

    # Tracking variables
    best_val_f1 = 0.0  
    epochs_without_improvement = 0
    best_epoch = 0
    best_val_acc = 0
    best_val_precision = 0 
    best_val_recall = 0
    best_val_loss = float('inf')  # Track best validation loss for reporting

    # Epoch extension
    original_num_epochs = num_epochs
    max_extensions = 2  
    extension_count = 0
    epochs_to_extend = 10 
    
    # Lists for metrics history
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    # Main training loop
    epoch = 0
    while epoch < num_epochs:
        
        # ===== TRAINING PHASE =====
        model.train()

        # Initialize metrics
        running_loss = 0.0 
        correct_train = 0 
        total_train = 0

        # Progress bar
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')

        # Loop through training batches
        for batch_data in train_pbar:
            # Unpack with donor labels
            inputs, labels, donor_ids = batch_data
            
            # Move data to device
            inputs, labels, donor_ids = inputs.to(device), labels.to(device), donor_ids.to(device)

            # Zero gradients
            optimizer.zero_grad()

            # Apply Mixup augmentation (80% probability)
            if np.random.rand() > 0.2:
                inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha=0.4)
                
                # Apply label smoothing to mixup targets
                labels_a_smooth = labels_a.float() * (1 - label_smoothing) + label_smoothing / 2
                labels_b_smooth = labels_b.float() * (1 - label_smoothing) + label_smoothing / 2
                
                with autocast('cuda'):
                    # Forward pass with DANN
                    activity_pred, donor_pred = model(inputs, return_donor_pred=True)
                    
                    # Activity loss (mixup)
                    activity_loss = mixup_criterion(activity_loss_fn, activity_pred, labels_a_smooth, labels_b_smooth, lam)
                    
                    # Donor loss (no mixup - just use original donor_ids)
                    donor_loss = donor_loss_fn(donor_pred, donor_ids)
                    
                    # Combined loss (GRL already reverses donor gradients)
                    loss = activity_loss + lambda_donor * donor_loss
            else:
                # Regular training (20% of batches to maintain some hard examples)
                # Apply label smoothing: 0 â†’ 0.1, 1 â†’ 0.9
                labels_smooth = labels.float() * (1 - label_smoothing) + label_smoothing / 2
                
                with autocast('cuda'):
                    # Forward pass with DANN
                    activity_pred, donor_pred = model(inputs, return_donor_pred=True)
                    
                    # Activity loss
                    activity_loss = activity_loss_fn(activity_pred, labels_smooth.unsqueeze(1))
                    
                    # Donor loss
                    donor_loss = donor_loss_fn(donor_pred, donor_ids)
                    
                    # Combined loss
                    loss = activity_loss + lambda_donor * donor_loss

            # Backward pass
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            

            # Gradient clipping (tight control for DANN stability)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # Update weights
            scaler.step(optimizer)
            scaler.update()

            # Update running metrics
            running_loss += loss.item()
            # Track accuracy (use activity predictions)
            predicted = (torch.sigmoid(activity_pred) > 0.5).float()
            total_train += labels.size(0)
            correct_train += (predicted == labels.unsqueeze(1)).sum().item()
        
        # Calculate epoch training metrics
        epoch_train_loss = running_loss / len(train_loader)
        epoch_train_acc = correct_train / total_train
        
        # Append to history lists
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_acc)

        # ===== VALIDATION PHASE =====
        model.eval()
        
        # Validation metrics
        running_val_loss = 0.0
        all_val_preds = []  
        all_val_labels = [] 

        # Progress bar
        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]  ', 
                        leave=False, ncols=100)

        # Disable gradient computation
        with torch.no_grad():

            # Loop through validation batches
            for batch_data in val_pbar:
                # Unpack with donor labels
                inputs, labels, donor_ids = batch_data

                # Move to device
                inputs, labels = inputs.to(device), labels.to(device)

                # Forward pass (only activity predictions needed)
                activity_pred = model(inputs, return_donor_pred=False)

                # Calculate loss
                loss = activity_loss_fn(activity_pred, labels.unsqueeze(1))

                running_val_loss += loss.item()
                predicted = (torch.sigmoid(activity_pred) > 0.5).float()
                
                # Collect predictions and labels for metrics
                all_val_preds.extend(predicted.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())
                
                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})

            # Calculate epoch validation metrics
            epoch_val_loss = running_val_loss / len(val_loader)

            # Calculate precision, recall, f1
            from sklearn.metrics import precision_recall_fscore_support, accuracy_score
            epoch_val_acc = accuracy_score(all_val_labels, all_val_preds)
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_val_labels, all_val_preds, average='binary', zero_division=0
            )

            epoch_val_f1 = f1

            # Append to history lists
            val_losses.append(epoch_val_loss)
            val_accuracies.append(epoch_val_acc)

        # Step scheduler (CosineAnnealingLR - smooth decay each epoch)
        scheduler.step()
        
        # Get current learning rate
        current_lr = optimizer.param_groups[0]['lr']
        
        # Print epoch summary
        print(f"Epoch [{epoch+1:2d}/{num_epochs}] | "
            f"LR: {current_lr:.6f} | "
            f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | "
            f"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f} | "
            f"Val F1: {epoch_val_f1:.4f} | Val Prec: {precision:.4f} | Val Rec: {recall:.4f}")
    
        # ===== EARLY STOPPING & MODEL CHECKPOINTING =====
        if epoch_val_f1 > best_val_f1:

            # Update best metrics
            best_val_f1 = epoch_val_f1 
            best_epoch = epoch + 1
            best_val_acc = epoch_val_acc
            best_val_precision = precision 
            best_val_recall = recall
            best_val_loss = epoch_val_loss  # Track best loss when model is saved
            epochs_without_improvement = 0

            # Save model checkpoint
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': epoch_train_loss,
                'val_loss': epoch_val_loss,
                'train_acc': epoch_train_acc,
                'val_acc': epoch_val_acc,
                'val_f1': epoch_val_f1,      
                'val_precision': precision,   
                'val_recall': recall,        
                'fold': fold_name,
            }, f'saved_models/best_model_fold_{fold_name}.pth')

            print(f"  âœ“ New best model saved! (Val F1: {epoch_val_f1:.4f})")

            if (epoch + 1 >= num_epochs - 2 and 
                extension_count < max_extensions):
                num_epochs += epochs_to_extend
                extension_count += 1
                print(f"  ðŸ”„ Model still improving! Extending training by {epochs_to_extend} epochs")
                print(f"     New max epochs: {num_epochs} (extension {extension_count}/{max_extensions})")

        else:
            epochs_without_improvement += 1
            print(f"  âš  No improvement for {epochs_without_improvement} epoch(s)")
            
            # Early stopping check
            if epochs_without_improvement >= patience:
                print(f"\nðŸ›‘ Early stopping triggered! No improvement for {patience} epochs.")
                print(f"   Best Val F1: {best_val_f1:.4f} at epoch {best_epoch}")
                break
            
        epoch += 1

    print(f"\nâœ… Training completed after {epoch} epochs")
    print(f"   Original budget: {original_num_epochs} epochs")
    if extension_count > 0:
        print(f"   Extended {extension_count} time(s) for {extension_count * epochs_to_extend} extra epochs")
    print(f"Best model was at epoch {best_epoch} with Val F1: {best_val_f1:.4f} (Precision: {best_val_precision:.4f}, Recall: {best_val_recall:.4f})")

    # ===== FINAL TEST EVALUATION (AFTER TRAINING COMPLETES) =====
    print(f"\n{'='*80}")
    print(f"FINAL TEST EVALUATION ON {fold_name}")
    print(f"{'='*80}")

    # Load best model
    checkpoint = torch.load(f'saved_models/best_model_fold_{fold_name}.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Create test dataset and loader (with donor labels for consistency)
    test_donor_labels = [extract_donor_id(f) for f in test_files]
    test_dataset = CellDataset(test_files, test_labels, augment=False, donor_labels=test_donor_labels)
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size * 4,  # 4x larger batch for testing (no gradients needed)
        shuffle=False,
        num_workers=4,
        pin_memory=True if torch.cuda.is_available() else False,
        prefetch_factor=4
    )

    # Test-Time Augmentation (TTA) Helper
    def predict_tta(model, inputs):
        """Apply test-time augmentation and average predictions"""
        model.eval()
        predictions = []
        
        # Original
        with torch.no_grad():
            pred = torch.sigmoid(model(inputs, return_donor_pred=False))
            predictions.append(pred)
        
        # Rotate 90Â°
        with torch.no_grad():
            rotated = torch.rot90(inputs, k=1, dims=[3, 4])  # Rotate HÃ—W plane
            pred = torch.sigmoid(model(rotated, return_donor_pred=False))
            predictions.append(pred)
        
        # Flip Horizontal
        with torch.no_grad():
            flipped_h = torch.flip(inputs, dims=[4])  # Flip width
            pred = torch.sigmoid(model(flipped_h, return_donor_pred=False))
            predictions.append(pred)
        
        # Flip Vertical
        with torch.no_grad():
            flipped_v = torch.flip(inputs, dims=[3])  # Flip height
            pred = torch.sigmoid(model(flipped_v, return_donor_pred=False))
            predictions.append(pred)
        
        # Average all predictions
        avg_pred = torch.stack(predictions).mean(dim=0)
        return avg_pred

    # Dynamic Threshold Tuning with K-Means
    def tune_threshold(model, val_dataset, device, batch_size):
        """Find optimal threshold using K-Means clustering on validation probabilities"""
        from sklearn.cluster import KMeans
        
        print("\nðŸ”§ Tuning decision threshold with K-Means...")
        
        # Create fresh val_loader for threshold tuning
        val_loader_tune = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        model.eval()
        all_probs = []
        
        with torch.no_grad():
            for batch_data in tqdm(val_loader_tune, desc='Collecting val probabilities', leave=False):
                inputs, _, _ = batch_data  # Unpack donor labels (unused)
                inputs = inputs.to(device)
                # Use TTA for more robust probabilities
                probs = predict_tta(model, inputs)
                all_probs.extend(probs.cpu().numpy().flatten())
        
        # Reshape for K-Means (needs 2D array)
        probs_array = np.array(all_probs).reshape(-1, 1)
        
        # K-Means with k=2 (active vs inactive clusters)
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        kmeans.fit(probs_array)
        
        # Get cluster centers
        centers = sorted(kmeans.cluster_centers_.flatten())
        inactive_center = centers[0]
        active_center = centers[1]
        
        # Optimal threshold = midpoint between clusters
        optimal_threshold = (inactive_center + active_center) / 2.0
        
        print(f"  Inactive cluster center: {inactive_center:.4f}")
        print(f"  Active cluster center:   {active_center:.4f}")
        print(f"  âœ“ Optimal threshold:     {optimal_threshold:.4f}")
        
        return optimal_threshold

    # Tune threshold on validation set
    optimal_threshold = tune_threshold(model, val_dataset, device, batch_size)

    # Testing with TTA and Dynamic Threshold
    print(f"\nTesting on holdout set (with TTA + threshold={optimal_threshold:.4f}):")
    test_loss = 0.0
    correct_test = 0
    total_test = 0
    all_predictions = []
    all_labels = []
    all_probs = []  # Store probabilities for analysis
    
    model.eval()
    for batch_data in tqdm(test_loader, desc='Testing (TTA)'):
        inputs, labels, _ = batch_data  # Unpack donor labels (unused)
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Use TTA for predictions
        probs = predict_tta(model, inputs)
        predicted = (probs > optimal_threshold).float()  # Use dynamic threshold
        
        # Calculate loss on original (non-augmented) prediction
        with torch.no_grad():
            outputs = model(inputs, return_donor_pred=False)
            loss = activity_loss_fn(outputs, labels.unsqueeze(1))
            test_loss += loss.item()
        
        total_test += labels.size(0)
        correct_test += (predicted == labels.unsqueeze(1)).sum().item()
        
        all_predictions.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs.cpu().numpy())

    final_test_loss = test_loss / len(test_loader)
    final_test_acc = correct_test / total_test

    from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
    cm = confusion_matrix(all_labels, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_predictions, average='binary', zero_division=0
    )
    
    print(f"\nðŸ“Š Test Results:")
    print(f"  Decision Threshold: {optimal_threshold:.4f} (K-Means optimized)")
    print(f"  Test Loss: {final_test_loss:.4f}")
    print(f"  Test Accuracy: {final_test_acc:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    print(f"\n  Confusion Matrix:")
    print(f"    TN={cm[0,0]}, FP={cm[0,1]}")
    print(f"    FN={cm[1,0]}, TP={cm[1,1]}")

    # ===== ERROR ANALYSIS =====
    print(f"\n{'='*80}")
    print("ERROR ANALYSIS: Understanding the 15% Ceiling")
    print(f"{'='*80}")
    
    # Categorize predictions
    fp_indices = [i for i, (pred, true) in enumerate(zip(all_predictions, all_labels)) 
                  if pred == 1 and true == 0]
    tp_indices = [i for i, (pred, true) in enumerate(zip(all_predictions, all_labels)) 
                  if pred == 1 and true == 1]
    fn_indices = [i for i, (pred, true) in enumerate(zip(all_predictions, all_labels)) 
                  if pred == 0 and true == 1]
    tn_indices = [i for i, (pred, true) in enumerate(zip(all_predictions, all_labels)) 
                  if pred == 0 and true == 0]
    
    # Extract probabilities for each category
    fp_probs = [all_probs[i][0] for i in fp_indices]
    tp_probs = [all_probs[i][0] for i in tp_indices]
    fn_probs = [all_probs[i][0] for i in fn_indices]
    tn_probs = [all_probs[i][0] for i in tn_indices]
    
    print(f"\nðŸ”´ False Positives (Predicted Active, Actually Inactive):")
    print(f"  Count: {len(fp_indices)} ({len(fp_indices)/(cm[0,0]+cm[0,1])*100:.1f}% of all inactive)")
    print(f"  Average Confidence: {np.mean(fp_probs):.3f} Â± {np.std(fp_probs):.3f}")
    print(f"  Median Confidence: {np.median(fp_probs):.3f}")
    print(f"  Quartiles: [{np.percentile(fp_probs, 25):.3f}, {np.percentile(fp_probs, 50):.3f}, {np.percentile(fp_probs, 75):.3f}]")
    print(f"  Highly confident errors (>0.7): {sum(1 for p in fp_probs if p > 0.7)} ({sum(1 for p in fp_probs if p > 0.7)/len(fp_probs)*100:.1f}%)")
    print(f"  Borderline errors (0.5-0.6): {sum(1 for p in fp_probs if 0.5 <= p <= 0.6)} ({sum(1 for p in fp_probs if 0.5 <= p <= 0.6)/len(fp_probs)*100:.1f}%)")
    
    print(f"\nðŸŸ¢ True Positives (Predicted Active, Actually Active):")
    print(f"  Count: {len(tp_indices)}")
    print(f"  Average Confidence: {np.mean(tp_probs):.3f} Â± {np.std(tp_probs):.3f}")
    print(f"  Median Confidence: {np.median(tp_probs):.3f}")
    
    print(f"\nðŸŸ  False Negatives (Predicted Inactive, Actually Active):")
    print(f"  Count: {len(fn_indices)} ({len(fn_indices)/(cm[1,0]+cm[1,1])*100:.1f}% of all active)")
    print(f"  Average Confidence: {np.mean(fn_probs):.3f} Â± {np.std(fn_probs):.3f}")
    print(f"  Median Confidence: {np.median(fn_probs):.3f}")
    
    print(f"\nâšª True Negatives (Predicted Inactive, Actually Inactive):")
    print(f"  Count: {len(tn_indices)}")
    print(f"  Average Confidence: {np.mean(tn_probs):.3f} Â± {np.std(tn_probs):.3f}")
    
    # Diagnosis
    print(f"\nðŸ”¬ DIAGNOSIS:")
    fp_median = np.median(fp_probs)
    if fp_median < 0.55:
        print(f"  âœ“ FP median confidence ({fp_median:.3f}) is near threshold")
        print(f"    â†’ These are genuinely ambiguous cells (biological overlap)")
        print(f"    â†’ Model is correctly uncertain about borderline cases")
        print(f"    â†’ Ceiling is likely due to overlapping lifetime distributions")
    elif fp_median < 0.70:
        print(f"  âš  FP median confidence ({fp_median:.3f}) is moderate")
        print(f"    â†’ Mix of borderline cases and systematic errors")
        print(f"    â†’ Some false positives might be mislabeled in ground truth")
        print(f"    â†’ Consider manual review of high-confidence errors")
    else:
        print(f"  âŒ FP median confidence ({fp_median:.3f}) is high!")
        print(f"    â†’ Model is confidently wrong on many inactive cells")
        print(f"    â†’ Suggests systematic bias or missing features")
        print(f"    â†’ Need to investigate what makes these cells look 'active'")
    
    # Create error analysis plot
    plt.figure(figsize=(15, 5))
    
    # Plot 1: Confidence distributions
    plt.subplot(1, 3, 1)
    bins = np.linspace(0, 1, 30)
    plt.hist(fp_probs, bins=bins, alpha=0.6, label=f'False Positives (n={len(fp_indices)})', color='red', edgecolor='darkred')
    plt.hist(tp_probs, bins=bins, alpha=0.6, label=f'True Positives (n={len(tp_indices)})', color='green', edgecolor='darkgreen')
    plt.axvline(optimal_threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({optimal_threshold:.3f})')
    plt.xlabel('Prediction Confidence (Probability of Active)')
    plt.ylabel('Count')
    plt.title('Confidence Distribution: Active Predictions')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 3, 2)
    plt.hist(fn_probs, bins=bins, alpha=0.6, label=f'False Negatives (n={len(fn_indices)})', color='orange', edgecolor='darkorange')
    plt.hist(tn_probs, bins=bins, alpha=0.6, label=f'True Negatives (n={len(tn_indices)})', color='blue', edgecolor='darkblue')
    plt.axvline(optimal_threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({optimal_threshold:.3f})')
    plt.xlabel('Prediction Confidence (Probability of Active)')
    plt.ylabel('Count')
    plt.title('Confidence Distribution: Inactive Predictions')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 3: Error summary
    plt.subplot(1, 3, 3)
    categories = ['False\nPositives', 'True\nPositives', 'False\nNegatives', 'True\nNegatives']
    counts = [len(fp_indices), len(tp_indices), len(fn_indices), len(tn_indices)]
    colors_plot = ['red', 'green', 'orange', 'blue']
    bars = plt.bar(categories, counts, color=colors_plot, alpha=0.7, edgecolor='black')
    plt.ylabel('Count')
    plt.title('Prediction Categories')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Add count labels on bars
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{count}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'saved_models/error_analysis_{fold_name}.png', dpi=300, bbox_inches='tight')
    print(f"\nðŸ“Š Error analysis plot saved to: saved_models/error_analysis_{fold_name}.png")
    plt.close()

    print(f"\nFold {fold_name} Complete!")
    print(f"Best Validation Epoch: {best_epoch} | Val F1: {best_val_f1:.4f} | Val Acc: {best_val_acc:.4f}")
    print(f"Test: Acc={final_test_acc:.4f} | F1={f1:.4f}")

    # Return results
    return {
        'fold': fold_name,
        'best_epoch': best_epoch,
        'val_loss': best_val_loss,
        'val_acc': best_val_acc,
        'val_f1': best_val_f1,          
        'val_precision': best_val_precision,  
        'val_recall': best_val_recall,
        # Test results
        'test_loss': final_test_loss,
        'test_acc': final_test_acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm.tolist(),
        # Training history
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        # Metadata
        'num_train_samples': len(train_files),
        'num_val_samples': len(val_files),
        'num_test_samples': len(test_files),
    }

# ============================================================================
# VISUALIZATION
# ============================================================================

def plot_cross_validation_results(all_results, save_path='saved_models/cross_validation_results.png'):
    """Create comprehensive plots for all folds"""
    num_folds = len(all_results)
    
    fig = plt.figure(figsize=(16, 10))
    
    # 1. Accuracy comparison across folds
    plt.subplot(2, 3, 1)
    folds = [r['fold'] for r in all_results]
    test_accs = [r['test_acc'] for r in all_results]
    plt.bar(folds, test_accs, color='skyblue', edgecolor='navy')
    plt.axhline(y=np.mean(test_accs), color='r', linestyle='--', 
                label=f'Mean: {np.mean(test_accs):.4f}')
    plt.title('Test Accuracy per Fold')
    plt.ylabel('Accuracy')
    plt.xlabel('Test Fold')
    plt.legend()
    plt.ylim([0, 1])
    
    # 2. Loss comparison across folds
    plt.subplot(2, 3, 2)
    test_losses = [r['test_loss'] for r in all_results]
    plt.bar(folds, test_losses, color='lightcoral', edgecolor='darkred')
    plt.axhline(y=np.mean(test_losses), color='b', linestyle='--', 
                label=f'Mean: {np.mean(test_losses):.4f}')
    plt.title('Test Loss per Fold')
    plt.ylabel('Loss')
    plt.xlabel('Test Fold')
    plt.legend()
    
    # 3. Training curves for all folds (Accuracy)
    plt.subplot(2, 3, 3)
    for result in all_results:
        plt.plot(result['val_accuracies'], label=f"{result['fold']}", alpha=0.7)
    plt.title('Validation Accuracy Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. Training curves for all folds (Loss)
    plt.subplot(2, 3, 4)
    for result in all_results:
        plt.plot(result['val_losses'], label=f"{result['fold']}", alpha=0.7)
    plt.title('Validation Loss Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 5. Sample distribution
    plt.subplot(2, 3, 5)
    train_samples = [r['num_train_samples'] for r in all_results]
    test_samples = [r['num_test_samples'] for r in all_results]
    x = np.arange(len(folds))
    width = 0.35
    plt.bar(x - width/2, train_samples, width, label='Train', color='lightgreen')
    plt.bar(x + width/2, test_samples, width, label='Test', color='orange')
    plt.xlabel('Fold')
    plt.ylabel('Number of Samples')
    plt.title('Sample Distribution per Fold')
    plt.xticks(x, folds)
    plt.legend()
    
    # 6. Summary statistics text
    plt.subplot(2, 3, 6)
    plt.axis('off')
    test_accs = [r['test_acc'] for r in all_results]
    test_losses = [r['test_loss'] for r in all_results]
    summary_text = f"""
    6-Fold Cross-Validation Summary
    ================================
    
    Number of Folds: {num_folds}
    
    Test Accuracy:
      Mean: {np.mean(test_accs):.4f} Â± {np.std(test_accs):.4f}
      Min:  {np.min(test_accs):.4f} ({folds[np.argmin(test_accs)]})
      Max:  {np.max(test_accs):.4f} ({folds[np.argmax(test_accs)]})
    
    Test Loss:
      Mean: {np.mean(test_losses):.4f} Â± {np.std(test_losses):.4f}
      Min:  {np.min(test_losses):.4f} ({folds[np.argmin(test_losses)]})
      Max:  {np.max(test_losses):.4f} ({folds[np.argmax(test_losses)]})
    
    Average Training Time:
      {np.mean([r['best_epoch'] for r in all_results]):.1f} epochs
    """
    plt.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',
             verticalalignment='center')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nPlots saved to: {save_path}")
    plt.show()


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    
    import time
    start_time = time.time()
    
    # Configuration parameters
    NUM_EPOCHS = 75
    PATIENCE = 20  # Increased to survive 20-epoch restart cycles
    LR = 0.0005
    BATCH_SIZE = 32
    WEIGHT_DECAY = 1e-3
    
    QUICK_TEST = True
    
    # Set random seed for reproducibility
    RANDOM_SEED = 42
    torch.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    print(f"ðŸŽ² Random seed: {RANDOM_SEED}")
    
    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")
    print(f"â° Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Directory for saving models
    os.makedirs('saved_models', exist_ok=True)
    
    # Collect all data organized by folder
    print("="*80)
    print("COLLECTING DATA")
    print("="*80)
    dataset_dict = collect_data_by_folder()
    
    # Check if any datasets were found
    if len(dataset_dict) == 0:
        print("Error: No datasets found!")
        return
    
    # Print total file count
    total_files = sum(len(files) for files, _ in dataset_dict.values())
    print(f"\nTotal files across all datasets: {total_files}")
    
    # Holdout and validation datasets
    d5_data = dataset_dict.pop('isolated_cells_D5')
    d6_data = dataset_dict.pop('isolated_cells_D6')

    cv_donor_names = ['isolated_cells_D1', 'isolated_cells_D2', 
                      'isolated_cells_D3', 'isolated_cells_D4'] 

    # Configure based on test mode
    if QUICK_TEST:
        print("\n" + "âš ï¸ " * 20)
        print("âš ï¸  QUICK TEST MODE ENABLED")
        print("âš ï¸  - Skipping cross-validation entirely")
        print("âš ï¸  - Training directly on D1+D2+D3+D4")
        print("âš ï¸  - Validating on D6")
        print("âš ï¸  - Testing on D5 (complete holdout)")
        print(f"âš ï¸  - Using validated hyperparameters (LR={LR}, BS={BATCH_SIZE}, WD={WEIGHT_DECAY})")
        print("âš ï¸  - Expected time: ~30 minutes")
        print("âš ï¸  - Set QUICK_TEST = False for full 4-fold CV")
        print("âš ï¸ " * 20 + "\n")
        
        print(f"Expected models to train: 1 (D5 final test only)")
    else:
        print(f"\nFull 4-fold CV with fixed hyperparameters")
        print(f"Expected models to train: {len(cv_donor_names)} outer folds + 1 (D5 final)")
    
    # Initialize list to store all results
    all_results = []
    
    # ==== Leave-One-Donor-Out Cross-Validation ====
    if not QUICK_TEST:
        print("\n" + "="*80)
        print("CROSS-VALIDATION")
        print("="*80)

        # Loop through each dataset as the held-out test fold
        for fold_idx, test_donor in enumerate(cv_donor_names):
            
            fold_start_time = time.time()
            
            # Split into train/val/test
            available_training_donors = [donor for donor in cv_donor_names if donor != test_donor]
            
            # Need at least 2 donors for train+val split
            if len(available_training_donors) < 2:
                print(f"\nâš ï¸  Skipping fold {test_donor} - not enough donors for proper train/val/test split")
                print(f"    (Need at least 3 total donors, have {len(cv_donor_names)})")
                continue
            
            # Use first available donor as validation, rest for training
            val_donor = available_training_donors[0]
            train_donors = available_training_donors[1:]
            

            print(f"\n>>> Test Donor: {test_donor} (Fold {fold_idx+1}/{len(cv_donor_names)})")
            print(f"    Validation Donor: {val_donor}")
            print(f"    Training Donors: {train_donors}")
            print(f"    Using fixed hyperparameters: LR={LR}, BS={BATCH_SIZE}, WD={WEIGHT_DECAY}")

            # Collect data
            train_files = []
            train_labels = []
            for donor in train_donors:
                files, labels = dataset_dict[donor]
                train_files.extend(files)
                train_labels.extend(labels)
            val_files, val_labels = dataset_dict[val_donor]
            test_files, test_labels = dataset_dict[test_donor]


            # Train fold with fixed hyperparameters
            fold_result = train_one_fold(
                fold_name=test_donor,
                train_files=train_files,
                train_labels=train_labels,
                val_files=val_files,
                val_labels=val_labels,
                test_files=test_files,
                test_labels=test_labels,
                device=device,
                num_epochs=NUM_EPOCHS,
                patience=PATIENCE,
                batch_size=BATCH_SIZE,
                learning_rate=LR,
                weight_decay=WEIGHT_DECAY
            )
            
            # Store hyperparameters used in this fold
            fold_result['hyperparameters'] = {
                'learning_rate': LR,
                'batch_size': BATCH_SIZE,
                'weight_decay': WEIGHT_DECAY
            }
            all_results.append(fold_result)
            
            # Print fold completion time
            fold_time = time.time() - fold_start_time
            print(f"\nâœ“ Fold {fold_idx+1}/{len(cv_donor_names)} completed in {fold_time/60:.1f} minutes")
            
            # Estimate remaining time
            if fold_idx < len(cv_donor_names) - 1:
                avg_time_per_fold = (time.time() - start_time) / (fold_idx + 1)
                remaining_folds = len(cv_donor_names) - (fold_idx + 1)
                estimated_remaining = avg_time_per_fold * remaining_folds
                print(f"â±ï¸  Estimated time remaining: {estimated_remaining/3600:.2f} hours ({estimated_remaining/60:.1f} minutes)")
    else:
        print("\nâš ï¸  Quick test mode: Skipping cross-validation")
        print("   Going directly to D5 final test\n")
    
    # ==== Final D5 Generalization Test ====
    print("\n" + "="*80)
    print("FINAL GENERALIZATION TEST ON D5 (COMPLETE HOLDOUT)")
    print("="*80)
    
    # In quick test mode, use default hyperparameters
    if QUICK_TEST or len(all_results) == 0:
        print("\nUsing default hyperparameters (validated in literature):")
        best_overall_hyperparams = {
            'learning_rate': LR,
            'batch_size': BATCH_SIZE, 
            'weight_decay': WEIGHT_DECAY
        }
        print(f"  LR: {LR}, BS: {BATCH_SIZE}, WD: {WEIGHT_DECAY}")
    else:
        # Find best overall hyperparameters from outer CV results
        best_fold = max(all_results, key=lambda x: x['test_acc'])
        best_overall_hyperparams = best_fold['hyperparameters']
        
        print(f"\nBest hyperparameters from outer CV: {best_overall_hyperparams}")
        print(f"(From fold: {best_fold['fold']} with accuracy: {best_fold['test_acc']:.4f})")
    
    # Combine all 5 CV donors for training
    print(f"\nTraining on: D1, D2, D3, D4")
    print(f"Validation on: D6 (completely held out from CV)") 
    print(f"Testing on: D5 (never seen before)")
    
    final_train_files = []
    final_train_labels = [] 
    for donor in cv_donor_names:
        files, labels = dataset_dict[donor]
        final_train_files.extend(files)
        final_train_labels.extend(labels)
    
    # Use D6 for validation
    val_files, val_labels = d6_data

    # Get D5 test data
    d5_test_files, d5_test_labels = d5_data
    
    # Train final model on all CV donors with best hyperparameters
    print("\n" + "="*80)
    print("TRAINING FINAL MODEL ON ALL CV DATA (D1+D2+D3+D4)")
    print("Testing on D5 (Complete Holdout)")
    print("="*80)
    print("\n" + "="*80)
    print("TRAINING FINAL MODEL ON ALL CV DATA (D1+D2+D3+D4)")
    print("Testing on D5 (Complete Holdout)")
    print("="*80)
    
    d5_result = train_one_fold(
            fold_name="D5_FINAL_TEST",
            train_files=final_train_files,
            train_labels=final_train_labels,
            val_files=val_files,
            val_labels=val_labels,
            test_files=d5_test_files,  
            test_labels=d5_test_labels,
            device=device,
            num_epochs=NUM_EPOCHS,
            patience=PATIENCE,
            batch_size=best_overall_hyperparams['batch_size'],
            learning_rate=best_overall_hyperparams['learning_rate'],
            weight_decay=best_overall_hyperparams['weight_decay']
    )
    
    # Print final D5 results
    print("\n" + "="*80)
    print("FINAL D5 GENERALIZATION RESULTS")
    print("="*80)
    print(f"\nðŸ“Š Testing Results:")
    print(f"  D5 Test Accuracy: {d5_result['test_acc']:.4f}")
    print(f"  D5 Test F1:       {d5_result['f1']:.4f}")
    print(f"  D5 Test Loss:     {d5_result['test_loss']:.4f}")
    print(f"\nâš™ï¸  Training Details:")
    print(f"  Best Epoch: {d5_result['best_epoch']}")
    print(f"  Hyperparameters: {best_overall_hyperparams}")
    
    # Save D5 results separately
    d5_results_file = 'saved_models/d5_final_test_results.json'
    with open(d5_results_file, 'w') as f:
        d5_json = {
            'timestamp': datetime.now().isoformat(),
            # Test results
            'test_accuracy': float(d5_result['test_acc']),
            'test_f1': float(d5_result['f1']),
            'test_precision': float(d5_result['precision']),
            'test_recall': float(d5_result['recall']),
            'test_loss': float(d5_result['test_loss']),
            # Training details
            'best_epoch': int(d5_result['best_epoch']),
            'hyperparameters': best_overall_hyperparams,
            'training_donors': cv_donor_names,
            'num_train_samples': d5_result['num_train_samples'],
            'num_test_samples': d5_result['num_test_samples'],
            'train_losses': [float(x) for x in d5_result['train_losses']],
            'val_losses': [float(x) for x in d5_result['val_losses']],
            'train_accuracies': [float(x) for x in d5_result['train_accuracies']],
            'val_accuracies': [float(x) for x in d5_result['val_accuracies']]
        }
        json.dump(d5_json, f, indent=2)
    
    print(f"\nD5 results saved to: {d5_results_file}")

    # ==== Summary ====
    print("\n" + "="*80)
    print("TRAINING COMPLETE")
    print("="*80)
    
    if len(all_results) > 0:
        # Only show CV results if we did CV
        print("\nCross-Validation Results Summary:")
        print("-" * 100)
        print(f"{'Test Fold':<20} {'Best Epoch':<12} {'Val Acc':<12} {'Test Acc':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}")
        print("-" * 100)
        
        for result in all_results:
            print(f"{result['fold']:<20} {result['best_epoch']:<12} "
                  f"{result['val_acc']:<12.4f} {result['test_acc']:<12.4f} "
                  f"{result['precision']:<12.4f} {result['recall']:<12.4f} {result['f1']:<12.4f}")
        
        print("-" * 100)
        
        # Calculate overall statistics
        mean_test_acc = np.mean([r['test_acc'] for r in all_results])
        std_test_acc = np.std([r['test_acc'] for r in all_results])
        mean_test_loss = np.mean([r['test_loss'] for r in all_results])
        std_test_loss = np.std([r['test_loss'] for r in all_results])
        
        print(f"\nOverall Cross-Validation Performance:")
        print(f"  Accuracy:  {mean_test_acc:.4f} Â± {std_test_acc:.4f}")
        print(f"  Loss:      {mean_test_loss:.4f} Â± {std_test_loss:.4f}")
        print(f"  Precision: {np.mean([r['precision'] for r in all_results]):.4f} Â± {np.std([r['precision'] for r in all_results]):.4f}")
        print(f"  Recall:    {np.mean([r['recall'] for r in all_results]):.4f} Â± {np.std([r['recall'] for r in all_results]):.4f}")
        print(f"  F1-Score:  {np.mean([r['f1'] for r in all_results]):.4f} Â± {np.std([r['f1'] for r in all_results]):.4f}")
        
        # Save results to JSON
        results_file = 'saved_models/cross_validation_results.json'
        with open(results_file, 'w') as f:
            json_results = []
            for r in all_results:
                json_r = r.copy()
                json_r['train_losses'] = [float(x) for x in r['train_losses']]
                json_r['val_losses'] = [float(x) for x in r['val_losses']]
                json_r['train_accuracies'] = [float(x) for x in r['train_accuracies']]
                json_r['val_accuracies'] = [float(x) for x in r['val_accuracies']]
                json_results.append(json_r)
            
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'mean_test_accuracy': float(mean_test_acc),
                'std_test_accuracy': float(std_test_acc),
                'mean_test_loss': float(mean_test_loss),
                'std_test_loss': float(std_test_loss),
                'folds': json_results
            }, f, indent=2)
        
        print(f"\nCV results saved to: {results_file}")
        
        # Create visualization
        plot_cross_validation_results(all_results)
    else:
        print("\nâš ï¸  Quick test mode: Cross-validation was skipped")
        print("   Only D5 final test was performed")
        print("   Set QUICK_TEST=False to run full 4-fold CV")
    
    # Calculate and display total time
    end_time = time.time()
    total_time = end_time - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = int(total_time % 60)
    
    print("\n" + "="*80)
    print("COMPLETE!")
    print("="*80)
    print(f"â° Started at:  {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
    print(f"â° Finished at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
    print(f"â±ï¸  Total time:  {hours}h {minutes}m {seconds}s ({total_time/3600:.2f} hours)")
    print("="*80)


# ============================================================================
# SCRIPT
# ============================================================================

if __name__ == "__main__":
    main()
